# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sc5JEKm1o7n1WmASqIGEDokmgq52wLUi
"""

#!/usr/bin/env python3
import os
import fitz
import json
import re
import torch
import string
from collections import defaultdict
from langdetect import detect
from transformers import T5Tokenizer, T5ForConditionalGeneration

# ----------------------------
# Model init (no 8-bit, no LangChain)
# ----------------------------
model_name = 'google/flan-t5-small'
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(
    model_name,
    device_map='auto'
)
model.eval()

# Optional: tiny cache to avoid regenerating the same title text
_title_cache = {}

# ----------------------------
# Helpers
# ----------------------------
def _normalize_title(s: str) -> str:
    """Lowercase, collapse spaces, strip punctuation; used for de-dup checks."""
    s = re.sub(r"\s+", " ", s).strip().lower()
    return s.strip(string.punctuation)

def add_item(outline, seen, level, text, page):
    """
    Append an outline item only if it's not:
      1) a consecutive duplicate (same level+page+normalized text),
      2) a per-page duplicate at the same level.
    """
    norm = _normalize_title(text)
    # 1) Block consecutive duplicate
    if outline:
        last = outline[-1]
        if last["level"] == level and last["page"] == page and _normalize_title(last["text"]) == norm:
            return
    # 2) Block same-level duplicate within the same page
    if norm in seen[page][level]:
        return

    outline.append({"level": level, "text": text, "page": page})
    seen[page][level].add(norm)

def generate_heading(text: str) -> str:
    """
    Generate a short title for a block of text using FLAN-T5.
    Uses a small cache and no_grad for speed/stability.
    """
    base = text.replace('\n', ' ')
    try:
        lang = detect(text) if text.strip() else "en"
    except Exception:
        lang = "en"

    cache_key = (lang, hash(base))
    if cache_key in _title_cache:
        return _title_cache[cache_key]

    prompt = f"Generate a title in {lang}: {base}"
    with torch.no_grad():
        inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        outputs = model.generate(
            **inputs,
            max_new_tokens=30,
            num_beams=4,
            early_stopping=True,
            no_repeat_ngram_size=2
        )
        heading = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

    if not heading:
        # Fallback to first sentence or a short prefix
        heading = text.split('.', 1)[0] if '.' in text else text[:80]

    _title_cache[cache_key] = heading
    return heading

def explicit_numbering_level(para: str) -> int:
    """
    Detect explicit numbering at the start of the paragraph:
      "1"        -> level 1
      "1.2"      -> level 2
      "1.2.3"    -> level 3 (capped at 3)
      otherwise  -> 0
    """
    m = re.match(r'^(\d+(?:\.\d+)*)\s+', para)
    return min(m.group(1).count('.') + 1, 3) if m else 0

# ----------------------------
# Core processing
# ----------------------------
def process(pdf_path: str):
    doc = fitz.open(pdf_path)
    outline = []

    # Track duplicates per page & level
    # seen[page]["H1"|"H2"|"H3"] -> set of normalized titles
    seen = defaultdict(lambda: {"H1": set(), "H2": set(), "H3": set()})

    for idx in range(doc.page_count):
        page_num = idx + 1
        raw = doc[idx].get_text('text')
        paras = [p.strip() for p in re.split(r'\n\s*\n+', raw) if p.strip()]
        if not paras:
            continue

        # H1 selection
        if explicit_numbering_level(paras[0]) == 1:
            h1 = paras[0]
        else:
            h1 = generate_heading(raw)
        add_item(outline, seen, 'H1', h1, page_num)

        # H2 / H3 / generated H2 for non-numbered paragraphs
        for p in paras:
            lvl = explicit_numbering_level(p)
            if lvl == 1:
                # already handled as H1 candidate
                continue
            if lvl == 2:
                add_item(outline, seen, 'H2', p, page_num)
            elif lvl == 3:
                add_item(outline, seen, 'H3', p, page_num)
            else:
                summary = generate_heading(p)
                add_item(outline, seen, 'H2', summary, page_num)

    return outline

# ----------------------------
# CLI entrypoint
# ----------------------------
def main(input_dir: str = '/app/input', output_dir: str = '/app/output'):
    os.makedirs(output_dir, exist_ok=True)
    aggregated = {}

    for fname in os.listdir(input_dir):
        if not fname.lower().endswith('.pdf'):
            continue
        path = os.path.join(input_dir, fname)
        base = os.path.splitext(fname)[0]
        outline = process(path)

        out_file = os.path.join(output_dir, f'{base}.json')
        with open(out_file, 'w', encoding='utf-8') as f:
            json.dump({'outline': outline}, f, indent=4, ensure_ascii=False)

        aggregated[base] = outline

    # aggregated output.json
    agg_path = os.path.join(output_dir, 'output.json')
    with open(agg_path, 'w', encoding='utf-8') as f:
        json.dump(aggregated, f, indent=4, ensure_ascii=False)

    print('Batch processing complete.')

if __name__ == '__main__':
    main()

import os
import glob
import shutil

# Create the /app/input directory
os.makedirs('/app/input', exist_ok=True)

# Move PDF files from /content/ to /app/input
pdf_files = glob.glob('/content/*.pdf')
for pdf_file in pdf_files:
    shutil.move(pdf_file, '/app/input/')

print("Created /app/input and moved PDF files.")

# Commented out IPython magic to ensure Python compatibility.
# %pip install PyMuPDF langdetect transformers torch